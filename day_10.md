# ðŸ“˜ Day 10

## âœ… Topic Covered  
LangChain Agents, Google API Integration, and Prompt Evaluation Techniques

## ðŸ§  Summary  
Today we wrapped up Week 2 with a powerful session on **LangChain** and **agent-based prompt engineering**.  
We learned how to create custom agents that can search the web, access tools, and perform structured tasks by chaining prompts and functions.

We also explored **prompt evaluation techniques** â€” methods to test, score, and improve prompt quality based on output performance.

A major part of the session was practical â€” we built agents using **LangChain + Google Generative AI API**, and even connected them with external tools like **search engines**.

## ðŸ§ª Key Concepts Covered

| Concept                | Description                                                                |
|------------------------|----------------------------------------------------------------------------|
| LangChain              | Framework for chaining prompts, memory, and tools in AI workflows         |
| Agents                 | AI units that can reason, call tools, and perform autonomous actions       |
| Function Calling       | Structured calls to external tools or APIs based on prompt triggers       |
| Prompt Evaluation      | Systematic testing of prompt effectiveness (clarity, correctness, intent) |
| Google Search Integration | Enabling agents to pull real-time web results                           |

## ðŸ’» Activity
- Built a LangChain-based AI agent  
- Integrated Google Search into the agent's workflow  
- Evaluated different prompts using structured techniques  
- Experimented with chaining tasks via prompt templates  
- Created a personalized AI tool using LangChain + Google API

## ðŸ¤” Challenges Faced
- Chaining logic sometimes broke due to unclear output formats  
- Debugging multi-step prompts required careful tracking  
- API rate limits occasionally disrupted testing

## ðŸŽ¯ Key Takeaway  
Agents are the next step in prompt engineering â€” they think, act, and reason across tools, and prompt evaluation is what makes them reliable.

## ðŸ“ˆ Understanding Today: 9.4/10
