# 📘 Day 12

## ✅ Topics Covered
- **Retrieval-Augmented Generation (RAG)** framework
- Architecture of RAG: Retriever + Generator
- Vector databases and embeddings
- Difference between vanilla LLMs and RAG-based systems

## 🧠 Summary
Today we learned how to enhance language models with **retrieval capabilities**. RAG combines the generative power of LLMs with external knowledge from documents or databases, making answers more accurate and up-to-date.

We explored how RAG systems use **vector similarity search** to find relevant information before generating a response. This helps overcome the limitations of LLM memory and hallucinations.

We also got a high-level overview of how **embedding models** work and how tools like **FAISS** or **Pinecone** store and retrieve knowledge chunks.

## 🔍 New Concepts Learned
- What is RAG and why it's needed
- Components of RAG:
  - **Retriever**: fetches relevant documents
  - **Generator**: composes the answer based on those docs
- Use of vector databases
- Basic idea of chunking and semantic search

## 💻 Activity

| Task                            | Description                                              |
|---------------------------------|----------------------------------------------------------|
| Concept walkthrough             | Studied RAG workflow and architecture                    |
| Hands-on (low-code) example     | Tested simplified retrieval-based prompt flow            |
| Embedding generation demo       | Saw how text is converted into numerical vectors         |

## 🤔 Challenges Faced
- Grasping vector math behind similarity was tricky at first
- Mapping where RAG fits in comparison to normal prompting

## 🎯 Key Takeaway
**RAG is like giving your LLM a memory boost.** It connects AI to external data, improving relevance and trustworthiness.

## 📈 Understanding Today: 8.7/10
